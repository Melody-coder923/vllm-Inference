import torch
from tokenizers import Tokenizer
from transformers import AutoTokenizer, AutoModelForCausalLM


# ç»å…¸çš„å®Œæ•´æ¨ç†æµç¨‹
def classic_inference_step_by_step():
    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "gpt2"  #æŒ‡å®šæ¨¡å‹åç§°
    tokenizer = AutoTokenizer.from_pretrained(model_name)  #åˆ›å»ºå®ä¾‹,åŠ è½½åˆ†è¯å™¨, æ‰€è°“åŠ è½½å³åŒ…å«è®­ç»ƒå¥½çš„æƒé‡
    model = AutoModelForCausalLM.from_pretrained(model_name) # åŠ è½½æ¨¡å‹, æ‰€è°“åŠ è½½å³åŒ…å«å®Œæ•´è¯æ±‡è¡¨
    model.eval() #è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼ï¼ˆå…³é—­è®­ç»ƒç‰¹æ€§ï¼‰
"""
eval()ä½œç”¨:
1. å…³é—­dropoutï¼ˆè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼‰
2. å…³é—­batch normalizationçš„æ›´æ–°
3. ç¡®ä¿æ¨ç†ç»“æœçš„ä¸€è‡´æ€§
"""
"""
from_pretrained() = åˆ›å»ºå®ä¾‹ + åŠ è½½é¢„è®­ç»ƒèµ„æº + é…ç½®å‚æ•°
åŠ è½½æ—¶çš„é¢å¤–é…ç½®
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    cache_dir="./my_cache",          # è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
    use_fast=True,                   # ä½¿ç”¨å¿«é€Ÿtokenizer
    trust_remote_code=True           # ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    cache_dir="./my_cache",          # è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
    torch_dtype=torch.float16,      # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœå†…å­˜
    device_map="auto",               # è‡ªåŠ¨åˆ†é…GPU
    low_cpu_mem_usage=True          # ä½CPUå†…å­˜ä½¿ç”¨
)
"""
    # 2. æ–‡æœ¬é¢„å¤„ç†ï¼ˆåˆ†è¯ï¼‰
    prompt = "The future of AI is"
    # ç¼–ç ï¼ˆæ–‡æœ¬ â†’ token IDs)
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
"""
åŒºåˆ†:
æ–¹æ³•1: tokenize() - åªåˆ†è¯,  è°ƒè¯• - çœ‹åˆ†è¯ç»“æœ
tokens = tokenizer.tokenize(text)
print("tokenize():", tokens)
è¾“å‡º: ['Hello', ' world', '!']

æ–¹æ³•2: encode() - åˆ†è¯ + è½¬ID + ç‰¹æ®Štoken , æ¨¡å‹æ¨ç† - ğŸ”¥ æœ€å¸¸ç”¨
token_ids = tokenizer.encode(text)
print("encode():", token_ids)
è¾“å‡º: [15496, 995, 0] (åŒ…å«ç»“æŸç¬¦)

å¯ä»¥äº’ç›¸è½¬æ¢
text = "Hello world"

è·¯å¾„1: ç›´æ¥encode
token_ids = tokenizer.encode(text)

è·¯å¾„2: å…ˆtokenizeå†convert
tokens = tokenizer.tokenize(text)
token_ids = tokenizer.convert_tokens_to_ids(tokens)
æ³¨æ„ï¼šè¿™æ ·ä¸ä¼šè‡ªåŠ¨æ·»åŠ ç‰¹æ®Štokenï¼

åå‘è½¬æ¢
tokens = tokenizer.convert_ids_to_tokens(token_ids)
text = tokenizer.convert_tokens_to_string(tokens)
æˆ–è€…ç›´æ¥: text = tokenizer.decode(token_ids)
"""

    # 3. é€æ­¥ç”Ÿæˆï¼ˆautoregressive generationï¼‰
    max_new_tokens = 10
"""
å«ä¹‰ï¼šæœ€å¤šç”Ÿæˆ10ä¸ªæ–°çš„token
å¦‚ä½•æ€è€ƒè¿™ä¸ªè®¾ç½®çš„è€ƒè™‘å› ç´ ï¼š
1. ä»»åŠ¡éœ€æ±‚ - ä½ æƒ³è¦å¤šé•¿çš„å›ç­”ï¼Ÿ
2. è®¡ç®—æˆæœ¬ - ç”Ÿæˆè¶Šå¤šè¶Šæ…¢
3. è´¨é‡æ§åˆ¶ - ç”Ÿæˆå¤ªé•¿å®¹æ˜“å
max_new_tokens = 5   # çŸ­å¥ï¼š"bright and promising"
max_new_tokens = 20  # ä¸­ç­‰ï¼š"bright and will revolutionize many industries"
max_new_tokens = 100 # é•¿ç¯‡ï¼šå®Œæ•´æ®µè½
"""

    temperature = 0.8

"""
å«ä¹‰ï¼šæ§åˆ¶ç”Ÿæˆçš„"åˆ›é€ æ€§"ç¨‹åº¦
æ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
åŸå§‹æ¦‚ç‡ = [0.4, 0.3, 0.2, 0.1]  å¯¹åº”ä¸åŒtokençš„æ¦‚ç‡

temperatureè°ƒæ•´åï¼š
temperature = 0.1 (ä¿å®ˆ)
è°ƒæ•´æ¦‚ç‡ = [0.8, 0.15, 0.04, 0.01]  æ›´ç¡®å®šæ€§ï¼Œé€‰æœ€å¯èƒ½çš„

temperature = 0.8 (å¹³è¡¡)
è°ƒæ•´æ¦‚ç‡ = [0.45, 0.28, 0.18, 0.09]  # é€‚åº¦éšæœº

temperature = 2.0 (åˆ›é€ æ€§)
è°ƒæ•´æ¦‚ç‡ = [0.32, 0.28, 0.24, 0.16]  # æ›´éšæœºï¼Œæ›´æœ‰åˆ›æ„

å®Œæ•´çš„ç”Ÿæˆå‚æ•°
generation_config = {
    "max_new_tokens": 10,
    "temperature": 0.8,
    "top_p": 0.9,          # æ ¸é‡‡æ ·ï¼Œä¿ç•™æ¦‚ç‡ç´¯ç§¯90%çš„token
    "top_k": 40,           # åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„40ä¸ªtoken
    "do_sample": True,     # å¯ç”¨é‡‡æ ·ï¼ˆè€Œéè´ªå©ªè§£ç ï¼‰
    "repetition_penalty": 1.1,  # é¿å…é‡å¤
}
"""

    #åœ¨ä»»ä½•éœ€è¦ä¿®æ”¹å¼ é‡ä½†åˆè¦ä¿ç•™åŸå§‹æ•°æ®çš„åœºæ™¯ä¸‹ï¼Œéƒ½åº”è¯¥ä½¿ç”¨ clone()ï¼è¿™æ˜¯PyTorchç¼–ç¨‹çš„åŸºæœ¬æœ€ä½³å®è·µã€‚
    generated_ids = input_ids.clone() # åˆ›å»ºç‹¬ç«‹å‰¯æœ¬
    generation_time = time.time() #è®°å½•å½“å‰æ—¶é—´æˆ³ï¼Œç”¨äºæ€§èƒ½æµ‹é‡ï¼

    with torch.no_grad():
        for step in range(max_new_tokens):
            #print(f"\n--- Step {step + 1} ---")

            # 3a. å‰å‘ä¼ æ’­
            outputs = model(generated_ids)
"""
model() ä¸æ˜¯å†…ç½®æ–¹æ³• - æ˜¯Pythonçš„__call__é­”æ³•æ–¹æ³•,å®é™…æ‰§è¡Œ - è°ƒç”¨æ¨¡å‹çš„forwardæ–¹æ³•
- è‡ªåŠ¨æ‰§è¡Œå„ç§hook")
- è‡ªåŠ¨å¤„ç†è®­ç»ƒ/è¯„ä¼°æ¨¡å¼"
ä¸è¦è°ƒç”¨forward()  è·³è¿‡äº†hookæœºåˆ¶",å¯èƒ½å¯¼è‡´æ„å¤–è¡Œä¸º"

"""
            logits = outputs.logits  # [batch_size, seq_len, vocab_size]
"""
logitså±äº: outputså¯¹è±¡ï¼ˆè¿”å›å€¼ï¼‰
è®¿é—®: outputs.logits
å½¢çŠ¶: [batch_size, seq_len, vocab_size]
[batch_size, sequence_length, vocab_size]"
æ‰¹æ¬¡å¤§å° (ä¸€æ¬¡å¤„ç†å‡ ä¸ªå¥å­); åºåˆ—é•¿åº¦ (è¾“å…¥æœ‰å‡ ä¸ªtoken); è¯æ±‡è¡¨å¤§å° (GPT-2æœ‰50,257ä¸ªè¯)
[    1     ,       4        ,   50257   ]
æ•°å€¼å«ä¹‰ logits[0, 3, 1234] = 5.67
[0]: ç¬¬1ä¸ªæ ·æœ¬; [3]: ç¬¬4ä¸ªä½ç½®; [1234]: è¯æ±‡IDä¸º1234çš„è¯;  5.67: è¯¥è¯åœ¨è¯¥ä½ç½®çš„'å¾—åˆ†'
ä½¿ç”¨æ–¹å¼:è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits (ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªè¯)
last_logits = logits[0, -1, :]  # shape: [vocab_size]"
"""

    # 3b. å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
            next_token_logits = logits[0, -1, :]  # [vocab_size]
    #next_token_logits = logits[0, -1, :] æ‰“å°å‡ºæ¥æ˜¯ 1ç»´å¼ é‡

    # 3c. åº”ç”¨æ¸©åº¦é‡‡æ ·
            if temperature != 1.0:
                next_token_logits = next_token_logits / temperature

    # 3d. è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ, dim=-1 çš„ä¼˜åŠ¿ï¼šä¸ç®¡å¼ é‡æ˜¯å‡ ç»´ï¼Œéƒ½æŒ‡å‘æœ€åä¸€ç»´
            probs = torch.softmax(next_token_logits, dim=-1)
"""
# dim=-1 çš„ä¼˜åŠ¿ï¼šä¸ç®¡å¼ é‡æ˜¯å‡ ç»´ï¼Œéƒ½æŒ‡å‘æœ€åä¸€ç»´
tensor_1d = torch.randn(1000)          # [vocab_size]
tensor_2d = torch.randn(5, 1000)       # [batch, vocab_size]
tensor_3d = torch.randn(2, 5, 1000)    # [batch, seq, vocab_size]

# éƒ½ç”¨ dim=-1ï¼Œè‡ªåŠ¨é€‚åº”ä¸åŒæƒ…å†µ
probs_1d = torch.softmax(tensor_1d, dim=-1)  # åœ¨ç»´åº¦0ä¸Š
probs_2d = torch.softmax(tensor_2d, dim=-1)  # åœ¨ç»´åº¦1ä¸Š
probs_3d = torch.softmax(tensor_3d, dim=-1)  # åœ¨ç»´åº¦2ä¸Š
"""

    # 3e. é‡‡æ ·ä¸‹ä¸€ä¸ª token
            next_token_id = torch.multinomial(probs, num_samples=1) #è¿™é‡Œï¼æ ¹æ®æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©

"""
def deeper_meaning():
é‡‡æ ·çš„æ·±å±‚æ„ä¹‰:

æ¨¡æ‹Ÿä¸ç¡®å®šæ€§:
çœŸå®ä¸–ç•Œå……æ»¡ä¸ç¡®å®šæ€§
äººç±»è¡Œä¸ºä¸æ˜¯100%å¯é¢„æµ‹çš„
é‡‡æ ·è®©AIæ›´æ¥è¿‘äººç±»çš„'éšæœºæ€§'

åˆ›é€ æ€§æ¥æº:
åˆ›æ–°å¾€å¾€æ¥è‡ª'æ„å¤–'é€‰æ‹©"
ä½æ¦‚ç‡é€‰é¡¹æœ‰æ—¶å¸¦æ¥æƒŠå–œ"
é‡‡æ ·ä¸ºAIæä¾›äº†'çµæ„Ÿ'çš„å¯èƒ½æ€§"

é¿å…å±€éƒ¨æœ€ä¼˜:
è´ªå©ªé€‰æ‹©å®¹æ˜“é™·å…¥é‡å¤æ¨¡å¼
é‡‡æ ·æä¾›äº†'è·³å‡º'çš„æœºä¼š
è®©ç”Ÿæˆè¿‡ç¨‹æ›´åŠ çµæ´»

å¹³è¡¡æ€§:
æ—¢ä¸å®Œå…¨éšæœº (å™ªéŸ³)
ä¹Ÿä¸å®Œå…¨ç¡®å®š (æ— èŠ)"
åœ¨'åˆç†'ä¸'æƒŠå–œ'é—´æ‰¾å¹³è¡¡"

deeper_meaning()
"""


"""
é‡‡æ ·çš„å‚æ•°é€‰æ‹©
def sampling_parameters():
å½±å“é‡‡æ ·çš„å‚æ•°ï¼š
Temperature (æ¸©åº¦):
ä½æ¸© (0.1): é‡‡æ ·æ›´å€¾å‘é«˜æ¦‚ç‡è¯æ±‡
ä¸­æ¸© (0.8): å¹³è¡¡çš„é‡‡æ ·
é«˜æ¸© (2.0): æ›´å¤šéšæœºæ€§ï¼Œæ›´å¤š'å†’é™©'é€‰æ‹©"


Top-k é‡‡æ ·:
åªåœ¨å‰kä¸ªæœ€é«˜æ¦‚ç‡è¯æ±‡ä¸­é‡‡æ ·
é¿å…é€‰æ‹©è¿‡äºä¸åˆç†çš„è¯æ±‡


Top-p (Nucleus) é‡‡æ ·:
ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pæ—¶æˆªæ­¢"
åŠ¨æ€è°ƒæ•´å€™é€‰è¯æ±‡æ•°é‡


å®é™…ä½¿ç”¨:
åˆ›æ„å†™ä½œ: é«˜temperature + top-p
æŠ€æœ¯æ–‡æ¡£: ä½temperature + top-k
æ—¥å¸¸å¯¹è¯: ä¸­ç­‰temperature + multinomial

"""
    # 3f. æ·»åŠ åˆ°åºåˆ—ä¸­
            generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=-1) #æŠŠåˆšç”Ÿæˆçš„æ–°è¯åŠ åˆ°å·²æœ‰å¥å­çš„æœ«å°¾
"""
generated_idsï¼šå½¢çŠ¶ [1, seq_len] - [batch_size, sequence_length]
next_token_idï¼šå½¢çŠ¶ [1] - éœ€è¦å˜æˆ [1, 1]

éœ€è¦åŒ¹é…çš„ç»´åº¦ç»“æ„ï¼š
ç¬¬0ç»´ï¼šbatch_sizeï¼ˆæ‰¹æ¬¡å¤§å°ï¼‰
ç¬¬1ç»´ï¼šsequence_lengthï¼ˆåºåˆ—é•¿åº¦ï¼‰

æ‰€ä»¥æˆ‘ä»¬éœ€è¦ç»™ next_token_id æ·»åŠ  batch ç»´åº¦ã€‚

ä¸ºä»€ä¹ˆéœ€è¦ unsqueeze(0)ï¼Ÿ
è¿™é‡Œæœ‰ä¸ªç»´åº¦åŒ¹é…çš„é—®é¢˜ï¼š
next_token_idçš„å½¢çŠ¶ï¼štorch.multinomial()è¿”å›çš„æ˜¯[1]ï¼Œä¸€ä¸ª1ç»´å¼ é‡
generated_idsçš„å½¢çŠ¶ï¼š[1, seq_len]ï¼Œä¸€ä¸ª2ç»´å¼ é‡ï¼ˆæ‰¹æ¬¡ç»´åº¦ Ã— åºåˆ—ç»´åº¦ï¼‰
ç›´æ¥æ‹¼æ¥ä¼šå‡ºé”™ï¼Œå› ä¸ºç»´åº¦ä¸åŒ¹é…ã€‚unsqueeze(0)çš„ä½œç”¨æ˜¯ç»™next_token_idå¢åŠ ä¸€ä¸ªç»´åº¦ï¼š

åŸæ¥ï¼š[token_id] â†’ å½¢çŠ¶[1]
å¤„ç†åï¼š[[token_id]] â†’ å½¢çŠ¶[1, 1]

unsqueeze() å‚æ•°çš„å«ä¹‰
unsqueeze(dim) ä¼šåœ¨æŒ‡å®šä½ç½® dim æ’å…¥ä¸€ä¸ªå¤§å°ä¸º1çš„æ–°ç»´åº¦ã€‚
å…·ä½“ä¾‹å­
å‡è®¾ next_token_id çš„å½¢çŠ¶æ˜¯ [1]ï¼š
unsqueeze(0) - åœ¨ç¬¬0ä¸ªä½ç½®æ’å…¥ï¼š

åŸæ¥ï¼š[token_id] â†’ å½¢çŠ¶ [1]
ç»“æœï¼š[[token_id]] â†’ å½¢çŠ¶ [1, 1]
ç†è§£ï¼šåœ¨æœ€å‰é¢åŠ äº†ä¸€ä¸ªç»´åº¦

unsqueeze(1) - åœ¨ç¬¬1ä¸ªä½ç½®æ’å…¥ï¼š

åŸæ¥ï¼š[token_id] â†’ å½¢çŠ¶ [1]
ç»“æœï¼š[[token_id]] â†’ å½¢çŠ¶ [1, 1]
ç†è§£ï¼šåœ¨åé¢åŠ äº†ä¸€ä¸ªç»´åº¦

ç­‰ç­‰ï¼Œä¸¤ä¸ªç»“æœä¸€æ ·ï¼Ÿè¿™æ˜¯å› ä¸ºåŸå¼ é‡åªæœ‰1ç»´ï¼Œæ‰€ä»¥ unsqueeze(0) å’Œ unsqueeze(1) æ•ˆæœç›¸åŒã€‚

è®°ä½è¿™ä¸ªè§„å¾‹ï¼š
unsqueeze(0) = "åœ¨å‰é¢åŠ ç»´åº¦" = "åŠ æ‰¹æ¬¡ç»´åº¦"
unsqueeze(-1) = "åœ¨åé¢åŠ ç»´åº¦" = "åŠ ç‰¹å¾ç»´åº¦"
"""

"""
æ‹¼æ¥å‰ï¼š
generated_ids:     [[15496, 995, 318, 389]]  å½¢çŠ¶ï¼š[1, 4]
next_token_id:     [[1049]]                   å½¢çŠ¶ï¼š[1, 1]

æ‹¼æ¥æ“ä½œï¼šåœ¨dim=-1ï¼ˆæœ€åä¸€ç»´ï¼‰ä¸Šè¿æ¥
         â†“
ç»“æœï¼š    [[15496, 995, 318, 389, 1049]]    å½¢çŠ¶ï¼š[1, 5]

"""

"""
ä¸€ç»´å¼ é‡æ‹¼æ¥
[1, 2, 3] + [4] = [1, 2, 3, 4]
#   â†‘                     â†‘
# åŸåºåˆ—               æœ«å°¾æ·»åŠ 

# åªèƒ½ç”¨dim=0ï¼Œå› ä¸ºåªæœ‰è¿™ä¸€ä¸ªç»´åº¦

äºŒç»´å¼ é‡æ‹¼æ¥

[[1, 2, 3]] + [[4]] = [[1, 2, 3, 4]]
#    â†‘                        â†‘
#  åŸåºåˆ—                  æœ«å°¾æ·»åŠ 

# dim=0: ä¼šå˜æˆ [[1,2,3], [4]] â† é”™è¯¯ï¼šåˆ›å»ºæ–°è¡Œ
# dim=-1: ä¼šå˜æˆ [[1,2,3,4]]  â† æ­£ç¡®ï¼šå»¶é•¿åºåˆ—
"""

"""
torch.cat() çš„å·¥ä½œåŸç†
torch.cat()æ˜¯PyTorchçš„å¼ é‡æ‹¼æ¥å‡½æ•°ï¼š

ç¬¬ä¸€ä¸ªå‚æ•°ï¼šè¦æ‹¼æ¥çš„å¼ é‡åˆ—è¡¨,è¦æ‹¼æ¥çš„å¤šä¸ªå¼ é‡. è¦æ±‚ï¼šé™¤äº†æ‹¼æ¥ç»´åº¦å¤–ï¼Œå…¶ä»–ç»´åº¦å¤§å°å¿…é¡»ç›¸åŒ.
ç¬¬äºŒä¸ªå‚æ•°: æŒ‡å®šåœ¨å“ªä¸ªç»´åº¦ä¸Šè¿›è¡Œæ‹¼æ¥  dim=-1ï¼šåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šæ‹¼æ¥ï¼ˆä¹Ÿå°±æ˜¯åºåˆ—é•¿åº¦ç»´åº¦ï¼‰
"""

    # 3g. æ˜¾ç¤ºå½“å‰ç”Ÿæˆçš„æ–‡æœ¬
            current_text = tokenizer.decode(generated_ids[0])
            print(f"Current text: {current_text}")

    # æ£€æŸ¥æ˜¯å¦é‡åˆ°åœæ­¢æ¡ä»¶
            if next_token_id.item() == tokenizer.eos_token_id:
                break
    generation_time = time.time() - generation_time #å‘¼åº”å‰é¢,è®¡ç®—æ—¶é—´ç»“æŸ
    final_text = tokenizer.decode(generated_ids[0])
    return final_text
